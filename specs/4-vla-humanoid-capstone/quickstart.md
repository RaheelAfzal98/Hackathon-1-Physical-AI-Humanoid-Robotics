# Quickstart Guide: Vision-Language-Action (VLA) Module

## Overview
This quickstart guide will help you get started with the Vision-Language-Action (VLA) module, which combines vision, language, and action to enable autonomous humanoid robot behavior.

## Prerequisites
- Completion of Modules 1, 2, and 3
- Basic understanding of ROS 2 concepts
- Access to OpenAI API for Whisper and LLM capabilities
- Python 3.10+ environment
- Standard ROS 2 Humble Hawksbill installation

## Setting Up Your Environment

1. Create a new Python virtual environment:
```bash
python -m venv vla_env
source vla_env/bin/activate  # On Windows: vla_env\Scripts\activate
```

2. Install required packages:
```bash
pip install openai
pip install openai-whisper
pip install torch torchvision torchaudio
pip install numpy
pip install rclpy
```

3. Set up your OpenAI API key:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

## Understanding the VLA Pipeline

The VLA system follows this conceptual pipeline:

1. **Voice Ingestion**: Listen to user commands using OpenAI Whisper
2. **Cognitive Planning**: Process commands with LLMs to generate action plans
3. **Navigation**: Plan and execute movement in the environment
4. **Perception**: Identify and understand objects in the environment
5. **Manipulation**: Perform physical actions to achieve the goal

## Example: Simple Voice Command to Action

Here's a basic example of how to process a voice command and convert it to an action:

```python
import openai
import whisper
import rclpy
from rclpy.node import Node

class VLASystem(Node):
    def __init__(self):
        super().__init__('vla_system')
        self.whisper_model = whisper.load_model("base")
        
    def process_voice_command(self, audio_file_path):
        # Transcribe the audio using Whisper
        result = self.whisper_model.transcribe(audio_file_path)
        transcript = result["text"]
        confidence = result["text"].count(" ") / len(result["text"])  # Simplified confidence calculation
        
        # Use an LLM to generate a plan from the transcript
        plan = self.generate_plan_with_llm(transcript)
        
        # Execute the plan using ROS 2 actions
        self.execute_plan(plan)
        
        return transcript, confidence, plan
    
    def generate_plan_with_llm(self, command):
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that converts natural language commands into structured action plans for humanoid robots. Each plan should be a sequence of simple robot actions from this list: navigate_to, grasp_object, release_object, speak, wait."},
                {"role": "user", "content": f"Convert this command into a structured action plan: {command}"}
            ]
        )
        return response.choices[0].message.content
    
    def execute_plan(self, plan):
        # This is where you would implement the ROS 2 action execution
        # based on the plan generated by the LLM
        pass

def main():
    rclpy.init()
    vla_system = VLASystem()
    
    # Process a sample voice command
    transcript, confidence, plan = vla_system.process_voice_command("path/to/audio/file.wav")
    
    print(f"Transcript: {transcript}")
    print(f"Confidence: {confidence}")
    print(f"Plan: {plan}")
    
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Running the Autonomous Humanoid Capstone

The capstone project demonstrates all aspects of VLA in a complete scenario. To execute the capstone:

1. Ensure your robot is properly configured with navigation, perception, and manipulation capabilities
2. Load the environment map
3. Start the VLA system
4. Issue a voice command to the system
5. Monitor the pipeline execution through the status API

```python
from vla_capstone import AutonomousHumanoidCapstone

# Initialize the capstone project
capstone = AutonomousHumanoidCapstone(
    robot_name="humanoid_robot",
    environment_map="kitchen_map.yaml"
)

# Execute the capstone with a voice command
result = capstone.execute("Please navigate to the kitchen and bring me a glass of water")

# Monitor the progress
while result.status != "completed":
    print(f"Progress: {result.progress * 100}%")
    print(f"Current stage: {result.pipeline}")
    time.sleep(5)  # Check every 5 seconds

print("Capstone project completed!")
```

## Integration with Previous Modules

Your VLA system builds upon concepts from previous modules:

- Use ROS 2 communication patterns from Module 1
- Apply simulation techniques from Module 2
- Leverage AI capabilities from Module 3

## Next Steps

1. Complete the hands-on exercises in each chapter
2. Experiment with different voice commands
3. Fine-tune the LLM prompts for better planning
4. Test the system in various simulated environments
5. Validate the complete capstone project

## Troubleshooting

- If Whisper transcription fails, check audio format and quality
- If LLM planning is inconsistent, refine the prompt engineering
- If ROS 2 actions fail, verify robot capabilities and environment setup